% section/paper on sampling with sampled priors.
% \documentclass[preprint,linenumbers]{aastex631} # 6.31 fails with \align!!
\documentclass[linenumbers, onecolumn, resetfootnote]{aastex7}
%\documentclass[onecolumn]{aastex7}
\usepackage{xcolor}

\usepackage{natbib}
\usepackage{amsmath}
\usepackage{enumitem}

\shortauthors{Bernstein et al.}
\shorttitle{Sampling sampled priors}
\reportnum{DES-2025-0886}
\reportnum{FERMILAB-PUB-25-0362-PPD}


\newcommand{\ed}[1]{{\color{red}{#1}}}
\newcommand\gary[1]{{\color{red} \{\textbf{GMB}: #1\}}}

\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\eqq}[1]{Equation~(\ref{#1})}
%\tabletypesize{\footnotesize}
\newcommand{\vecc}{\ensuremath{\mathbf{c}}}
\newcommand{\vecq}{\ensuremath{\mathbf{q}}}
\newcommand{\vecs}{\ensuremath{\mathbf{s}}}
\newcommand{\vecn}{\ensuremath{\mathbf{n}}}
\newcommand{\vecu}{\ensuremath{\mathbf{u}}}
\newcommand{\vecU}{\ensuremath{\mathbf{U}}}
\newcommand{\vecv}{\ensuremath{\mathbf{v}}}
\newcommand{\hatc}{\ensuremath{\bar{\mathbf{c}}}}
\newcommand{\vecP}{\ensuremath{\mathbf{P}}}
\newcommand{\covm}{C}
\newcommand{\matA}{\ensuremath{A}}
\newcommand{\matD}{D}
\newcommand{\matE}{E}
\newcommand{\matF}{F}
\newcommand{\matG}{G}
\newcommand{\matI}{I}
\newcommand{\matT}{T}
\newcommand{\matX}{X}
\newcommand{\matY}{Y}
\newcommand{\matV}{V}
\newcommand{\matLam}{\Lambda}
\newcommand{\proj}{P}  % Projection matrix
\newcommand{\jac}{J}   % Jacobian
\newcommand{\ident}{I}  % identity
\newcommand{\DD}{\Delta_D}
\newcommand{\likeli}{\mathcal{L}}
\newcommand{\trace}{\text{Tr}}


\begin{document}

\title{Sampling posterior distributions when only samples from a prior are available}


\include{authors.tex}
\correspondingauthor{G.~M.~Bernstein}

\begin{abstract}
	\vspace{0.2in}
A typical Bayesian inference on the values of some parameters of
interest $\vecq$ from some data $D$ involves running a Markov Chain (MC) to sample
from the posterior $p(\vecq,\vecn | D) \propto \likeli(D | \vecq,\vecn)
p(\vecq) p(\vecn),$ where $\vecn$ are some nuisance parameters \ed{with
  separable prior.}  In \ed{some}
cases, the nuisance parameters are high-dimensional, and their
prior $p(\vecn)$ is itself defined only by a set of samples that have
been drawn from some other MC.  Two problems arise: first,
the MC for the posterior will typically require evaluation of
$p(\vecn)$ at arbitrary values of $\vecn,$ \ie\ one needs to provide a
density estimator over the full $\vecn$ space from the provided
samples.  Second, the high dimensionality of $\vecn$ hinders both the
density estimation and the efficiency of the MC for the posterior.  We
describe a solution to this problem: a linear compression of the
$\vecn$ space into a much lower-dimensional space $\vecu$ which
projects away directions in $\vecn$ space that cannot appreciably
alter $\likeli.$ The algorithm for doing so is a slight modification
to principal components analysis, and is less restrictive on
$p(\vecn)$ than other proposed solutions to this issue.
We demonstrate this ``mode projection'' technique using the analysis
of 2-point correlation functions of weak lensing fields and galaxy
density in the \textit{Dark Energy Survey}, where $\vecn$ is a binned representation of the redshift distribution
$n(z)$ of the galaxies.
\end{abstract}


\section{Motivation} \label{sec:intro}

Consider an inference in which we have a vector of observable summary
statistics \vecc\ that we are using to constrain a set of parameters of
interest \vecq.  There is a model $\hatc(\vecq,\vecn)$ for the
\ed{expectation value of the} observables which involves the parameters of interest, but also a
vector \vecn\ of nuisance parameters.  
We wish to characterize the Bayesian posterior \ed{density}
\begin{equation}
  p(\vecq | \vecc) \propto \int dn\, \likeli(\vecc | \vecq, \vecn) p(\vecq) p(\vecn),
\label{eq:posterior}
\end{equation}
where \ed{$\likeli(\vecc | \vecq, \vecn)$ is a known likelihood function
of the data, and we assume that the priors on $\vecq$ and $\vecn$ are separable
to $p(\vecq)$ and $p(\vecn)$.}\footnote{Following the nomenclature for the
elements of Bayes's formula in, \eg, \url{https://en.wikipedia.org/wiki/Posterior_probability}.}
This posterior is complex enough that it requires
approximation by the output of a Markov Chain (MC) wandering across the space $(\vecq,\vecn).$
\ed{Implicit in \eqq{eq:posterior} is that the prior $p(\vecn)$ is independent of
  the likelihood $\likeli$ for $\vecc,$ \eg\ the prior on $\vecn$ has been constrained using
  data that are distinct from those entering the likelihood.}

The scenario we address here is when \emph{the prior $p(\vecn)$ is \ed{not
    available in evaluable form, rather we have only
  a set of samples of $\vecn$ known to be drawn} from this
  distribution,} Most MC samplers require that the posterior (and
hence the prior and likelihood) be an
evaluable function of any value of the parameters. It is the general task of density estimators to convert the samples of $\vecn$ into an evaluable $p(\vecn).$  But when \vecn\ is of high dimension, two problems arise: first, there may be insufficient available samples to create a viable density estimator; second, sampling of the posterior in (\ref{eq:posterior}) becomes infeasible if the MC must traverse a high-dimensional space.

A concrete example, which motivated this paper's work,
is when the observable data \vecc\ are the binned 2-point correlation functions of
cosmic fields derived from a catalog of galaxies; the parameters of
interest are cosmological quantities such as the  matter density
$\Omega_m,$ the amplitude of density fluctuations $\sigma_8,$
etc.; and the nuisance parameters $\vecn$
include the coefficients of some linear expansion of the redshift distributions $n(z)$ of the galaxies being observed:
\begin{equation}
  n(z) = \sum_{k=1}^{N} n_k b_k(z).
  \label{eq:nzbasis}
\end{equation}
The $b_k$ are a set of predetermined basis functions for the redshift
distribution, \ed{\eg\ these are boxcar functions if we are modeling $n(z)$ as
  stepwise constant.}  In our case of analyzing the data from the  \textit{Dark Energy Survey} (DES), there are 10 distinct samples of galaxies---each designed to prefer galaxies in a limited range of redshift---which we can index by $s.$ Each has its own $n_s(z)$ to be characterized by coefficients $n_{sk}$ at $\approx100$ values of $k$, leading to $N=O(1000)$ parameters $n_{sk}$ to be considered.  The vector $\vecn$ of nuisance parameters would be the concatenation of all the $n_{sk}.$ For clarity, we will still write this as $\vecn=\{n_1,n_2,\ldots,n_N\}$ and demonstrate the method with a single galaxy sample's $n(z)$.

One approach would be to run a new MC over \vecq\ for each of the samples
we have of \vecn, and then concatenate these to effect marginalization over
\vecn.  This is clearly infeasible if a \ed{thousands or more} of \vecn\ samples are needed to characterize the prior in this space.

Facing this problem for the cosmological analyses of the 3-year data
(Y3) from DES,
\citet{hyperrank} devised a scheme whereby
the samples of \vecn, \ed{which we write as $\{\vecn_\alpha\}$ for $\alpha\in
  1\ldots N_\alpha,$} are placed in a grid within some $M$-dimensional 
unit hypercube $\mathcal{H}$.  The coordinates \vecu\ within the hypercube
are considered the compressed parameters of $n(z),$ and the
decompression function
$\hat{\vecn}(\vecu)$ outputs the $\vecn_\alpha$ sample at the nearest grid point to
  \vecu.  This solves the problem of creating a continuous \vecu\
  domain, and \ed{gives each sample $\vecn_\alpha$ equal probability under
  $p(\vecu),$ maintaining the meaning of the input samples.}
But the \ed{\emph{output} of the function $\hat\vecn(\vecu)$,} and the resultant likelihood
  function \ed{$\likeli(\vecc | \vecq, \vecu),$ are discontinuous over $\vecu.$}
Various strategies are proposed by \citet{hyperrank} to assign the $\vecn_\alpha$ to the grid points in
$\mathcal{H}$ based on summary statistics, to reduce the
discontinuities---but the function is never smooth.
As a consequence, many MC samplers become quite inefficient in
sampling of the cosmological posterior.  In particular, samplers such
as \textsc{MultiNest} that assume continuity are rendered nearly
non-functional.  As a result, the Y3 cosmological priors could not be
evaluated with this method.  Instead, the \vecn\ samples were not
used, and an \textit{ad hoc} $p(\vecn)$ was adopted which allowed only
shifts and dilations of the mean $n(z)$ of the \vecn\ samples [see \eqq{eq:zs}].

A more rigorous and extremely efficient method of marginalizing over high-dimensional nuisance parameters was 
described by \citet{bridle02} and reprised by \citet{hans} for the $n(z)$ application, for the case where the following restrictions apply:
\begin{enumerate}
\item The likelihood of the observable \vecc\ is normal, $\vecc \sim \mathcal{N}( \hatc, \covm_c),$ with $\covm_c$ fixed.
\item The prior $p(\vecn)$ can also be assumed to be normal, with a mean taken to be $\bar\vecn = \left\langle\vecn\right\rangle$ and covariance matrix taken to be $\covm_n=\left\langle(\vecn-\bar\vecn)(\vecn-\bar\vecn)^T\right\rangle$ using the samples of \vecn\ we are given.
\item The model \hatc\ can be linearized in \vecn\ about fiducial values $\vecq_0, \vecn_0$ without loss of accuracy exceeding measurement errors, with the derivatives independent of \vecq.
\end{enumerate}
Under these conditions, the marginalization over \vecn\ is \ed{shown to be
  algebraically} equivalent to adding terms to $\covm_c,$ such that any MC process need not sample \vecn\ at all.  

We describe here an approach that is algebraically similar to this analytic marginalization,
but does not require the 2nd condition of Gaussianity for
the nuisance prior.
%, and is likely to be a good approximation when the first and third conditions
%are not grossly violated.
Our approach is to seek a linear compression of \vecn\ into a lower-dimensional
set of parameters \vecu\ that projects away variations in \vecn\ that do not
influence the likelihood $\likeli.$  Standard density estimators can then be
applied to the \vecu\ values implied by the known \vecn\ samples to yield a
prior $p(\vecu)$ that can be used for the MC of the cosmological posterior.  The
model \hatc, and hence $\likeli,$ will be continuous over this low-dimensional
\vecu\ space, and marginalization over \vecu\ will yield posterior probabilities
very close to marginalization over the original \vecn.  \ed{In the example
  application described in Section~\ref{sec:app}, an 80-dimensional \vecn\ is
  compressed into a 3-dimensional space.}

\section{Derivation}\label{sec:deriv}
We assume that we do have a multivariate normal likelihood for the
observables \vecc\, with the mean being some model
$\hatc(\vecq,\vecn)$ and a fixed, \ed{known} covariance matrix $\covm_c.$ In this
section we will assume that the \vecn\ vectors have been shifted by
the mean of the samples $\vecn_0 \equiv \left\langle \vecn_\alpha
\right\rangle,$ \ed{generating a replacement set of $n_\alpha$ that have zero mean.}
We seek some function $\hat{\vecn}(\vecu)$ of a much lower-dimensional
vector \vecu\ which can be substituted for \vecn\ and yield nearly the
same likelihood function for any \vecn\ in the domain spanned by the
samples $\{\vecn_\alpha\}.$
This means we want maps $\vecn_\alpha\rightarrow
\vecu_\alpha \rightarrow \hat{\vecn}_\alpha,$ with ${\rm dim}(\vecn)={\rm dim}(\hat{\vecn})=N$ and ${\rm dim}({\vecu})=M\ll N.$ We wish to find maps such that replacing $\vecn$ with
$\hat\vecn$ alters the cosmological inference by much less than the
other uncertainties in the model or data.  We will implement this by minimizing the
distance in the space $\vecc$ between the
model generated by $\vecn$ and that by $\hat\vecn$, using the
observations' covariance matrix $\covm_c$ as a metric for the
distance.\footnote{More precisely, the inverse of the covariance
  matrix is the metric.}  This is equivalent to the $\chi^2$ of the difference
between the original and compressed models for the data:
\begin{equation} \left\langle \chi^2 \right\rangle
=  \frac{1}{N_\alpha} \sum_\alpha
                                            \left[ \hatc(\vecq,\vecn_\alpha) - \hatc(\vecq,\hat{\vecn}_\alpha) \right]^T
                                            \covm_c^{-1}
                                            \left[ \hatc(\vecq,\vecn_\alpha) - \hatc(\vecq,\hat{\vecn}_\alpha) \right].
\label{eq:chisq}
\end{equation}
If the data are in fact drawn from the model $\hatc(\vecq,\vecn)$ with
a Gaussian likelihood, then this is also the mean shift in $-2\log\likeli$
from the compression.  It is \emph{not}, however, equal to the mean
shift of the overall log of the posterior in
\eqq{eq:posterior}---rather, $\left\langle\chi^2\right\rangle$ is serving as a
proxy for the true log-likelihood shift.  \ed{It is true, at least, that as $\langle
\chi^2 \rangle \rightarrow 0,$ that the substitution of $\hat\vecn$ for $\vecn$
has no effect on the calculated posterior density, so can be considered a
lossless compression as far as inference on \vecq\ is concerned.}

We next assume that the compression is linear, $\hat\vecn=\matX\vecn,$
for some $N\times N$ matrix that is idempotent ($\matX\matX = \matX$).  We will further linearize the dependence of $\hat\vecc$ on $\vecn,$ specifically assuming that (in scalar notation) 
\begin{equation}
    \frac{\partial^2 \hat c}{\partial n^2} n \ll \frac{\partial \hat c}{\partial n}
\end{equation}
over the full range of variation of $\vecn.$  
With these two assumptions, \eqq{eq:chisq} becomes
\begin{equation}
  \left\langle \chi^2 \right\rangle = \frac{1}{N_\alpha} \sum_\alpha
  \left[ (\matI-\matX)\vecn_\alpha\right]^T \matF  \left[ (\matI-\matX)\vecn_\alpha\right].
  \label{eq:linearized}
\end{equation}
We use the Jacobian matrix of the model $\hatc$ to define
\begin{equation}
  \matF \equiv
  \left[\frac{\partial\hatc}{\partial\vecn}\right]_{\vecq_0, \vecn_0}^T
  \covm_c^{-1} \left[\frac{\partial\hatc}{\partial\vecn}\right]_{\vecq_0,
    \vecn_0}.
\label{eq:fisher}
\end{equation}
This quantity is also the Fisher matrix giving the information
provided by the observations $\vecc$ about the nuisance parameters
\vecn.  In many cases this matrix will be
rank-deficient and/or poorly conditioned, since the observables are not
likely to be very informative on $\vecn$---if they were, we might not be
concerned with establishing a prior on $\vecn$ to begin with.
Fortunately, we will not require the inverse of $\matF$ in our algorithm.

The optimization implied by \eqq{eq:linearized} is the same as in
familiar Principal Components Analysis (PCA), aside from the presence
of the $F$ matrix, which in essence defines a new metric for the
variance to be captured by the principal components.  Our solution will follow the typical derivation
for PCA, but with an additional variable transformation to compensate
for the presence of $\matF.$

Since $\matX$ is idempotent, we can write
\begin{equation}
  \matX = V_X \proj_M V_X^T,
\end{equation}
where $V_X$ is unitary and the projection matrix $\proj_M$ is defined as
\begin{equation}
  \left(\proj_M\right)_{ij} \equiv
\begin{cases}
                                            1,  &  i=j\le M \\
                                            0,  & \text{otherwise}
\end{cases}
\end{equation}
It is also useful to define
\begin{equation}
  \matY \equiv \matI-\matX  = V_X \proj_{-M} V_X^T,
\end{equation}
with $\proj_{-M}=\matI-\proj_M.$  

For a chosen rank $M$ of the
transformation matrix $X$, our task becomes to identify the
eigenvectors $V_X$ that minimize
\begin{align}
  \left\langle \chi^2\right\rangle & = \frac{1}{N_\alpha} \sum_\alpha
  \left[ \matY \vecn_\alpha\right]^T \matF  \left[\matY
                                     \vecn_\alpha\right] \\
       & = \trace \left[ \covm_n V_X \proj_{-M} V_X^T \matF V_X
         \proj_{-M} V_X^T \right], \\
  C_n & \equiv \left\langle \vecn \vecn^T \right\rangle.
\end{align}
This optimization is easier if we first transform the systematic variables to
$\vecn^\prime =\matT\vecn$ such that $\covm_{n^\prime}=\matI$, \ie\
make the elements of $\vecn$ uncorrelated and unit-variance.  This
is accomplished by finding the eigensystem $\covm_n=\matV_n \matLam_n
\matV_n^T$ and setting $\matT = \matLam_n^{-1/2} \matV_n^T$.  With
this transformation, we are now seeking a different unitary matrix
$\matV_{X^\prime}$ that minimizes
\begin{align}
  \left\langle \chi^2\right\rangle & = \trace \left[ \matI
    V_{X^\prime} \proj_{-M} V_{X^\prime}^T \left[ (T^{-1})^T \matF
      T^{-1} \right] V_{X^\prime} 
    \proj_{-M} V_{X^\prime}^T \right] \\
  & = \trace \left[ \proj_{-M} V_{X^\prime}^T \matG 
    V_{X^\prime} \proj_{-M} \right],
  \label{eq:mintr}
\end{align}
where we have defined the transformed Fisher matrix
\begin{equation}
  \matG \equiv \left(T^{-1}\right)^T \matF   T^{-1} = \matLam_n^{1/2} \matV_n^T
  \matF \matV_n \matLam_n^{1/2} = \matV_G \matLam_G \matV_G^T.
  \label{eq:defG}
\end{equation}
The right-hand side defines the eigensystem of $G$, with $\Lambda_G={\rm diag}(\lambda^G_1,\ldots,\lambda^G_N),$ and $\lambda^G_i\ge0.$  \eqq{eq:mintr} can now be rewritten as
\begin{equation}
\left\langle \chi^2\right\rangle = \sum_{i>M} \left(\matV\lambda_G \matV^T\right)_{ii}
\end{equation}
where $\matV=\matV^T_{X^\prime}\matV_G$ is unitary.  This expression must be at least as large as sum of the $N-M$ smallest $\lambda^G_i$, and that minimum is attained if
$\matV_{X^\prime}^T \matV_G = \matI \; \Rightarrow \; \matV_{X^\prime} = \matV_G,$ and the eigensystem of $G$ is placed
in order of decreasing eigenvalues $\lambda^G_i.$ The
elements surviving the projection $P_{-M}$ yield
\begin{equation}
  \left\langle \chi^2\right\rangle = \sum_{i>M} \lambda_i^G.
  \label{eq:chiresid}
\end{equation}
In other words each eigenvalue of the matrix $\matG$ in \eqq{eq:defG}
gives the contribution to $\left\langle\chi^2\right\rangle$ of one
projection (mode) of $\vecn.$

Transforming the solution back into the space of $\vecn$ yields
\begin{align}
  \matX & = \matT^{-1} \matV_G \proj_M \matV_G^T \matT \\
\label{eq:DE}
   & = \left[ \matV_n^T \matLam_n^{1/2} \matV_G \proj_M \right] \left[
     \proj_M \matV_G^T \matLam_n^{-1/2} \matV_n^T \right] \\
        & \equiv \matD \matE.
\end{align}
We thus obtain our optimal encoding/compression using the nonzero rows of matrix
$\matE$ to give
\begin{equation}
  \vecu_\alpha = \matE \vecn_\alpha
  \label{eq:uu}
\end{equation}
and the decoding/reconstruction of the systematic variables as
\begin{equation}
  \hat\vecn_\alpha = \matD \vecu_\alpha.
  \label{eq:UU}
\end{equation}
One can confirm that this procedure yields a compressed representation
$\vecu$ such that $\covm_u = \matI_M,$ the $M$-dimensional
identity matrix.

The previous derivation ignores the possibility that $\covm_n$ is
singular or nearly so, such that taking $\matLam_n^{-1/2}$ in
\eqq{eq:DE} is not possible.  Indeed in our application, it is
\emph{required} that $\covm_n$ be singular, because we have a sum
normalization constraint on the initial $\vecn_\alpha$ values.  Any
such (nearly) zero element $j$ of $\matLam_n$ has a corresponding
eigenvector $\vecv_j$ of the $\vecn$ space which has zero amplitude
in all of the input samples $\vecn_\alpha,$ so that the $\vecn$ are
confined to a subspace---therefore the reconstructed $\hat\vecn$ should also be.
The compressed
representations $\vecu_\alpha$ and reconstructed $\hat\vecn_\alpha$
should therefore be unaffected by the presence of any $\vecv_j$
component.  This can be accomplished in \eqq{eq:DE} by setting element
$j$ of $\matLam_n^{-1/2}$ to zero, as is typically done during
solutions of least-squares problems using singular value
decompositions.

In summary, the procedure for dimensional reduction is:
\begin{enumerate}
  \item Obtain the mean $\vecn_0$ of the input samples, the Fisher matrix $\matF$ of the system defined in
    \eqq{eq:fisher} using derivatives about $\vecn_0,$ 
    plus the covariance matrix $\covm_n$ of the samples.
  \item From the eigensystem $(\matLam_n,\matV_n)$ of $C_n$, form the
    matrix $G$ defined in \eqq{eq:defG} and get its eigensystem
    $(\matLam_G,\matV_G).$  Place the eigenvalues in descending order.
  \item Choose the size $M$ of the compressed representation to be the
    minimum that keeps the $\left\langle\chi^2\right\rangle$ value in
    \eqq{eq:chiresid} below a chosen threshold, presumably $\ll 1.$ \ed{If this
      is achieved, the multiplicative errors in the likelihood induced by the
      compression are at percent
      levels, so errors in the posterior likelihoods are bounded to the same level.}
  \item The encoding matrix $E$ and decoding matrix $D$ are formed as
    in \eqq{eq:DE}, taking the inverse $\matLam_n^{-1/2}$ to be zero
    for any eigenvalues that are zero (or within roundoff errors).
  \item Compress all incoming (mean-subtracted) samples using
    \eqq{eq:uu}.  The resulting $\vecu$ values will have
    have unit covariance matrix and zero mean.
  \item Construct a continuous density estimator in $\vecu$ space that
    mimics the finite sample distribution.  If the distribution is
    normal, this becomes the multidimensional unit normal.  There
    is, however,  no \textit{a priori} reason that this must be the case,
    and something like a normalizing flow may be needed to
    approximate this lower-dimensional representation of the prior.
  \item Sample over $\vecu$ space in the Markov Chain that is sampling the
    posterior on the parameters of interest $\vecq,$ using \eqq{eq:UU}
    to transform each sample back into a $\hat\vecn$ vector.
  \end{enumerate}
  
The ability to accomodate non-Gaussian distributions of the
nuisance-parameter space is the principle
advantage of our method over the single-step covariance-inflation
method of \citet{hans}.  A further advantage of our
compression$+$density-estimator scheme over covariance inflation is
that nonlinearities in the data model, including presence of
significant $\partial^2\hatc/\partial\vecq\partial\vecn,$ will be
correctly handled in the inference to the extent that the higher
derivatives are confined to the compressed subspace of $\vecn.$ 
But even in the case where linearity and gaussianity in \vecu\ hold,
there are practical advantages of compressing the nuisance 
variables and retaining them in the cosmological Markov chain rather
than using the covariance-inflation method for analytic
marginalization.  One can, for example, examine the posterior
distributions of \vecu\ to see how the data have constrained the prior.  For example, if the cosmological posterior for $\vecu$ is at the edge of the prior, this would potentially indicate an inconsistency between the data and
the prior.
  

\section{Application}\label{sec:app}
As an example of the application of this straightforward dimensional
reduction to a high-dimensional nuisance parameter, we examine the
redshift distribution of one of the bins of ``Maglim''  galaxies used
as a lens population and clustering tracer in the Year 6 (Y6) analysis of
the DES galaxy catalogs.  Each of these Maglim ``lens bins''
is selected with cuts on galaxy fluxes and
colors in an attempt to generate a sample that is confined to a
particular redshift range, as described in \citet{y6maglim}.  Once each bin's
\ed{galaxy selection criteria are} chosen, a combination of 
photometric techniques \citep{y6lenspz,y6pz} and clustering information
\citep{y6wz} is used to generate samples from the posterior
distribution of  \ed{$n(z)$ functions applicable to the bin members,} conditioned
on the photometric and clustering data, \ed{\ie\ the $p(\vecn)$ that will become the
  prior for the inference of \vecq.}

In the simplest case,
there are 6 cosmological parameters of interest, $\vecq = \{\Omega_m,
\Omega_b, \sigma_8, h, n_s, m_\nu\}.$   The nuisance vector $\vecn$
has $\approx 5$  parameters in each of the following categories: galaxy
biases with respect to matter; intrinsic alignments of galaxy shapes
with the tidal field of the mass; magnification coefficients; and multiplicative errors in the
measurement of galaxy shear, for a total of 24 non-redshift parameters.  The $n(z)$ for each galaxy bin
is
described by \eqq{eq:nzbasis} with 80 coefficients spanning $0<z<4$ at
intervals of $\Delta z=0.05.$ If all of these coefficients, for each
of the 10 galaxy selections, were allowed to vary, the parameter space for inference would grow from 24 to 824 dimensions.
The inference would become infeasible even if a
reliable density estimator over the 80-dimensional space could be
created from the $O(10^4)$ samples available to characterize each
$n(z).$ 

We hence turn to the modal projection technique herein to reduce the
dimensionality to those directions in $\vecn$ space in which the samples span a large
enough range to alter the $\hatc(\vecn)$ at \ed{levels comparable to the
  precision of the observations.}

The observable quantities $\vecc$ whose modeling depends upon these nuisance
parameters are the angular autocorrelation $w(\theta)$ of the 
galaxies, and the cross-correlations $\gamma_t^{(s)}(\theta)$ between
these galaxies' positions and the weak gravitational lensing shear
observed from groups $1\le s \le 4$ of source galaxies.  The
derivatives of the model for each observable, and the expected
covariance, are calculated from theory with the tools described in
\citet{y6model} and the \texttt{Cosmosis} software\footnote{\url{https://cosmosis.readthedocs.io/en/latest/}} \citep{cosmosis}.

\begin{figure}
  \center
  \includegraphics[width=0.6\textwidth]{bin4_violins.pdf}
\caption{Violin plots for the redshift probability distribution $n(z)$
  of galaxies in lens bin 4 for the DES Y6 analysis.  The orange regions
  show the distributions for the samples of $\vecn$ derived from
  photometric and clustering information.  The blue violins are for
  $\vecn$ values drawn defined by (1) subtracting the mean $\bar\vecn$; (2) compressing these $\vecn$
  into 3 modes with coefficients $\vecu;$ (2) drawing values of
  $\widetilde{u}_i$ from unit normal distributions; (3) transforming each
  component $\widetilde u_i$ to match the 1d distribution of the input samples' $u_i$; (4) decompressing the transformed $u_i$ draws back into full-length $\vecn$ samples; finally (5) restoring the mean $\bar\vecn.$  The dashed
  line connects the mean values of $n(z),$ which are the same for
  generated samples as for the input samples, by construction.  The compression substantially lowers the variance of $n(z)$ at individual values of $z$ without significantly altering the variation of cosmological signals that the entire $n(z)$ predicts.
  [Although the $n(z)$ functions are calculated out to $z=4,$ we
  truncate this and other plots at lower $z$ to emphasize the
  lower-redshift regime where this bin's galaxies are primarily found.]}
  \label{fig:violins}
\end{figure}

We present results for mode-compression sampling of the $n(z)$
parameters for redshift bin 4 of the DES Y6 lens galaxies.  Figure~\ref{fig:violins}
plots the distributions of the individual elements of $n(z)$ in the
input 3000 samples.  As per the procedure described in this paper, the
mean \vecn\ is subtracted from each sample and the covariance
$\covm_n$ computed.  This is then combined with the
derivatives and covariance matrices of the observables $\vecc$ to
calculate the decomposition in \eqq{eq:defG}.

\begin{figure}
  \center
  \includegraphics[width=0.49\textwidth]{chiresids.pdf}
   \includegraphics[width=0.49\textwidth]{modes.pdf}
\caption{At left: The size of the $\chi^2$ of modeling error attributable to compressing the
  $\vecn$ samples down to $M$ modes is plotted vs $M$.  The $M=0$ point shows the modelling error from holding $n(z)$ fixed at its mean, and $M\ge1$ values drop exponentially as we use more modes to reconstruct $n(z).$ Our chosen criterion of
  $\chi^2<0.025$ is attained with $M=3$ for this bin's
  $n(z).$
  At right: The modes of variation $U_i(z),$ \ie\ the rows of the decompression
  matrix $\matD$ in \eqq{eq:DE}, are plotted vs redshift.  Each of
  modes 0,1,2 is
  multiplied by a unit-variance stochastic coefficient $u_i$, then
  they are summed with the mean $\bar\vecn(z),$ to form an $n(z)$
  sample.  Higher-numbered modes have
  observable consequences of decreasing statistical significance. Mode 7 is plotted as the dashed blue line as an example of what is projected out of $n(z)$; even though its typical amplitude in the input data is larger than modes 1 or 2, its oscillatory behavior does not lead to measurable changes in the cosmological statistics.}
  \label{fig:chiresid}\end{figure}

The left plot in Figure~\ref{fig:chiresid} shows the values of unmodelled
$\chi^2$ vs the dimension $M$ of the compressed space, as per
\eqq{eq:chiresid}.  This modelling error induced by compression drops
exponentially with the number of retained modes.  We have somewhat arbitrarily
chosen a threshold of $\chi^2<0.025$ for each galaxy sample in order to keep the
total impact of compression \ed{of 10 samples} to $\ll 1.$ This is attained with $M=3$ modes for this bin.  The right side of Figure~\ref{fig:chiresid} plots the individual modes
${\rm U}_i(z),$ \ie\ the rows of the decompression matrix $\matD$ such
that $\hat\vecn = \bar\vecn + \sum_{i\le M} u_i \vecU_i.$ Recall that
each mode's coefficient $u_i$ will be a random deviate with unit variance. Each of the three modes appears to effect some combination of a $z$ shift of the main $n(z)$ peak, a change of the peak's shape/width, and a change in the low-$z$ contamination. We have also plotted mode 7 in the Figure, to illustrate a mode of variation that is present in the samples, but has unobservable consequences.  Mode 7 is more oscillatory in redshift than the three retained modes, and does not alter the low-$z$ tail.


\begin{figure}
  \center
  \includegraphics[width=0.7\textwidth]{bin4_corner.pdf}
  \caption{In orange is a corner plot of the distribution of the mode coefficients,
    \ie\ the elements of $\vecu = \matE \vecn.$
  of the input samples after encoding.  The coefficients, especially 
  $u_1,$ are significantly skewed so a normal distribution would be an
  inaccurate model.  Instead we model each $u_i$ as an
  ``denormalizing'' function of a unit-normal variable, as per
  \eqq{eq:denorm}.  The blue histograms and contours show the
  distributions obtained using this element-by-element transformation
  technique, which is seen to accurately reproduce the distribution of
  the input samples.}
\label{fig:corner}
\end{figure}

Figure~\ref{fig:corner} plots the distributions
of the $M=3$ components of the compressed $\vecu$ representations of the
3000 input samples, \ie\ the vectors $\vecu_j=E(\vecn_j-\bar\vecn)$ obtained from each input sample $\vecn_j$.  Some of the components of $\vecu$ have substantially non-Gaussian distributions,
so a unit-normal prior on $\vecu$ will not faithfully represent the
input samples---a better density estimator is required.  We find in
this case (and in all other DES cases) that it is sufficient to
normalize the marginal distributions of the individual $u_i$
components.  This is done by tabulating a normalizing function $f_i$
for each mode defined by
\begin{equation}
  {\rm CDF}_n\left[ f_i(u_i) \right] = {\rm CDF}(u_i),
\label{eq:denorm}
\end{equation}
where the left side is the cumulative distribution function of the
unit normal, and the right-hand side is the CDF of the $u_i$ values
obtained from the input samples.  The functions $f_i$ are bijective
and can be stored as a splined lookup table.
Now, the cosmological Markov Chain
is told that there are 3 parameters in $\widetilde{\vecu}$ that have a
unit-normal prior $p(\widetilde{\vecu}).$  This vector defines the
redshift distribution via a 2-step process:
\begin{align}
  u_i & =f_i^{-1}(\widetilde{u}_i); \\
  \vecn & = \matD \vecu.
\end{align}

The blue violins in Figure~\ref{fig:violins} show the distributions
of the $n(z)$ values that are generated by drawing $\widetilde{\vecu}$
values from a unit Gaussian.  It is clear now that projecting away the
unobservable fluctuations in $n(z)$ has substantially reduced the
variance of the function at any \emph{individual} $z$ value, but the \emph{collective} $n(z)$ behavior still retains the same observable influence on the summary statistics $\vecc$.  To check whether we have achieved our goal
of leaving the observable consequences of the $n(z)$ variation
unchanged, we calculate the distribution of
\begin{equation}
  \chi^2 =  \left[\hatc(\vecq,\vecn) - \hatc(\vecq,\vecn_0) \right]^T
                                           \covm_c^{-1}  \left[ \hatc(\vecq,\vecn) - \hatc(\vecq,\vecn_0)\right]
\label{eq:chihist}
\end{equation}
for the cases when (1) the $\vecn$ are drawn from the input samples, vs
(2) are generated using the procedure defined above.  This $\chi^2$
measures the deviation of the model from that implied by the mean
vector $\vecn_0$ at some chosen nominal value of cosmological
parameters $\vecq$ (we also fix the other nuisance parameters of the
DES model for this test).

\begin{figure}
  \center
  \includegraphics[width=0.6\textwidth]{bin4_chisq.pdf}
\caption{The histograms show the deviations of the predicted observable $w(\theta)$ quantities in
  DES Y6 cosmological analysis, as measured by the $\chi^2$ in
  \eqq{eq:chihist}, as we allow the $n(z)$ parameters to vary.  The
  shaded green histogram shows the variation using the original 3000 samples of
  $n(z)$ produced by the photometric and clustering redshift studies.
  The dashed yellow histogram results from
  drawing 3-dimensional $\widetilde{\vecu}$ values from a unit
  normal and decompressing them into $n(z)$ realizations using our
  method.  This lower-dimensional model for the prior on $n(z)$
  reproduces the original samples' result very well.  By comparison,
  the dash-dotted red histogram generates samples of $n(z)$ using
  the \textit{ad hoc} method of \eqq{eq:zs}, which reproduces the
  observable behavior of the original samples very well.  This method's two
  parameters $z$ and $s$ are given priors to match the distributions
  of the mean of $n(z)$ over $z$, and the standard deviation of $z$,
  present in the input samples.  The \textit{ad hoc} method produces
  $n(z)$ fluctuations with $\approx20\%$ larger $\chi^2$ from the mean,
  on average, than the input distribution has.}
\label{fig:chihist}
\end{figure}

Figure~\ref{fig:chihist} shows the results: the $\chi^2$ distribution
for the input samples is indistinguishable from that of the samples
generated by our compressed, normalized representation.
The Figure also shows the $\chi^2$ distribution resulting from samples
generated by the method used in the DES Y3 analysis \citep{y3pz}.  In
that case, the $n(z)$ for each galaxy sample was given an \textit{ad
  hoc} variation of the form
\begin{equation}
  n(z) = n_0\left(\frac{z-\Delta\,z}{s}\right),
  \label{eq:zs}
\end{equation}
with $\Delta z$ and $s$ being ``shift'' and ``stretch'' parameters.
Separable Gaussian priors were assigned to $\Delta z$ and $s$ with
means of 0 and 1, respectively, and standard deviations that equaled
the RMS variation in the mean and width of the input $n(z)$ samples.
The shift-stretch model essentially compresses the samples of $n(z)$
into two parameters, their mean and standard deviation, and executes
an \textit{ad hoc} reconstruction based on those parameters.
In the Figure we can see that the shift-stretch model does not in fact
reproduce the size of the deviations in the $\vecc$ observables that
is implied by the original samples; the mean $\chi^2$ induced by the
shift-stretch samples is $\approx20\%$ larger than the input samples.  
This mismatch indicates that some cosmologically relevant information in the
original samples is being lost, or that the shift-stretch samples are
imposing cosmological constraints that are not present in the original
samples.

For simplicity, we have demonstrated this method for a case in which
$\vecn$ specifies the $n(z)$ function for a single sample of DES
galaxies.  The method is fully applicable to any set of nuisance
parameters for which we are given a set of samples from $p(\vecn).$
For instance, in the DES Y6 analysis, we use an $\vecn$ that is a
concatenation of the parameters of $n(z)$ for 4 bins of lens source
galaxies.  Since each output sample from the  redshift-estimate
process describes all 4 bins, we know the cross-correlations between
the bins' $n(z)$ behavior, and the compression process described
herein will properly preserve these correlations in the subsequent
cosmological analysis.  In the DES Y6 analysis, we have also extended
the nuisance parameter vector $\vecn$ to include the multiplicative
errors on the shear measurement method, and create a compressed
$\vecu$ that captures correlations between the multiplicative errors
and the $n(z)$ estimates.

\section{Summary}
  We present a linear dimensionality reduction technique that has the
  aim of making it feasible to produce continuous density estimators
  for nuisance parameters that \ed{have no evaluable prior, and} are characterized only
  from a set of samples in 
  a high-dimensional space $\vecn.$  This is essentially a principle
  components analysis that is adjusted to separate contributions to
  the detectable consequences of $\vecn$ rather than contributions to
  the Euclidean norm of $\vecn.$   This method enables a rigorous
  marginalization over the distribution of $\vecn$ even for
  non-Gaussian distributions, as long as the derivatives of the
  log-likelihood of the data with respect to $\vecn$ are nearly
  constant over the posterior domain.  Indeed the effect of the
  nuisance parameters on the inference will be correctly calculated
  even if these derivatives vary across the posterior domain, as long
  as the statistically relevant values of $\vecn$ do not deviate
  significantly from the linear subspace defined by our compression.
The mode-projection technique
  is also useful as a method of projecting away irrelevant
  noise fluctuations in $\vecn.$

\ed{This technique was developed} for the case when
  $\vecn$ represents the redshift distributions of galaxies in DES.  \ed{Some
  attempts at sampling the posterior $p(\vecq| \vecc)$ while fully respecting
  the prior $p(\vecn)$ as embodied by the samples $\{\vecn_\alpha\}$ have
  proven infeasible, such as concatenating MC's run at every $\vecn_\alpha,$ or the
  nearest-neighbor hypercube embedding of \citet{hyperrank}.  The method of
  \citet{bridle02} that propagates $p(\vecn)$ into an inflated $\covm_c$ relies on
  assumptions of Gaussianity for the nuisances that are not necessarily valid
  here, and precludes some forms of internal consistency checks.  This forced
  previous DES analyses to create an \textit{ad hoc} family of parameterized
  $n(z)$ functions and priors in an attempt to mimic the cosmological impact of
  the true $p(\vecn)$ distribution.  The mode-projection technique we describe is
  able to do a better job, with
  principled adherence to the prior embodied by
  the input samples.
  Extensive
  use of this method on the 10 different galaxy samples defined for the Year 6
  analysis of DES data is reported by \citet{y6pz, y6maglim} and \citet{y6wz}.

While this new approach to marginalizing over $n(z)$  has minimal consequence for the
  posterior cosmological parameter estimates in the DES Y6 analysis,
  the mode-projection method is better motivated and will produce more
  accurate posteriors in future experiments where the $n(z)$
  prior's uncertainty is a dominant contributor to the cosmological posterior,
  and can be generally applicable to other analyses where critical nuisance parameters
  do not have evaluable priors.}


  \begin{acknowledgments}
G.M.B. acknowledges support from NSF grant AST-2205808 and DOE award DE-SC0007901.
W. dâ€™A. acknowledges support from the  MICINN projects PID2019-111317GB-C32, PID2022-141079NB-C32 as well as predoctoral program AGAUR-FI ajuts (2024 FI-1 00692) Joan Or\'o.
The project that gave rise to these results received the support of a fellowship to A. Alarcon from "la Caixa" Foundation (ID 100010434). The fellowship code is LCF/BQ/PI23/11970028.

Funding for the DES Projects has been provided by the U.S. Department of Energy, the U.S. National Science Foundation, the Ministry of Science and Education of Spain, 
the Science and Technology Facilities Council of the United Kingdom, the Higher Education Funding Council for England, the National Center for Supercomputing 
Applications at the University of Illinois at Urbana-Champaign, the Kavli Institute of Cosmological Physics at the University of Chicago, 
the Center for Cosmology and Astro-Particle Physics at the Ohio State University,
the Mitchell Institute for Fundamental Physics and Astronomy at Texas A\&M University, Financiadora de Estudos e Projetos, 
Funda{\c c}{\~a}o Carlos Chagas Filho de Amparo {\`a} Pesquisa do Estado do Rio de Janeiro, Conselho Nacional de Desenvolvimento Cient{\'i}fico e Tecnol{\'o}gico and 
the Minist{\'e}rio da Ci{\^e}ncia, Tecnologia e Inova{\c c}{\~a}o, the Deutsche Forschungsgemeinschaft and the Collaborating Institutions in the Dark Energy Survey. 

The Collaborating Institutions are Argonne National Laboratory, the University of California at Santa Cruz, the University of Cambridge, Centro de Investigaciones Energ{\'e}ticas, 
Medioambientales y Tecnol{\'o}gicas-Madrid, the University of Chicago, University College London, the DES-Brazil Consortium, the University of Edinburgh, 
the Eidgen{\"o}ssische Technische Hochschule (ETH) Z{\"u}rich, 
Fermi National Accelerator Laboratory, the University of Illinois at Urbana-Champaign, the Institut de Ci{\`e}ncies de l'Espai (IEEC/CSIC), 
the Institut de F{\'i}sica d'Altes Energies, Lawrence Berkeley National Laboratory, the Ludwig-Maximilians Universit{\"a}t M{\"u}nchen and the associated Excellence Cluster Universe, 
the University of Michigan, NSF NOIRLab, the University of Nottingham, The Ohio State University, the University of Pennsylvania, the University of Portsmouth, 
SLAC National Accelerator Laboratory, Stanford University, the University of Sussex, Texas A\&M University, and the OzDES Membership Consortium.

Based in part on observations at NSF Cerro Tololo Inter-American Observatory at NSF NOIRLab (NOIRLab Prop. ID 2012B-0001; PI: J. Frieman), which is managed by the Association of Universities for Research in Astronomy (AURA) under a cooperative agreement with the National Science Foundation.

The DES data management system is supported by the National Science Foundation under Grant Numbers AST-1138766 and AST-1536171.
The DES participants from Spanish institutions are partially supported by MICINN under grants PID2021-123012, PID2021-128989 PID2022-141079, SEV-2016-0588, CEX2020-001058-M and CEX2020-001007-S, some of which include ERDF funds from the European Union. IFAE is partially funded by the CERCA program of the Generalitat de Catalunya.

We  acknowledge support from the Brazilian Instituto Nacional de Ci\^encia
e Tecnologia (INCT) do e-Universo (CNPq grant 465376/2014-2).

This document was prepared by the DES Collaboration using the resources of the Fermi National Accelerator Laboratory (Fermilab), a U.S. Department of Energy, Office of Science, Office of High Energy Physics HEP User Facility. Fermilab is managed by Fermi Forward Discovery Group, LLC, acting under Contract No. 89243024CSC000002.
\end{acknowledgments}

\textbf{Contributions:}
GB devised the compression method, wrote the relevant code, and wrote the article text.  MT derived the relevant derivative matrices, and Wd'A applied the method to the DES Maglim bin and produced figures.  Authors Wd'A, MT, AA, AA, GG, and BY developed the redshift samples used in the demonstration, integrated the code into DES pipelines, and advised on the implementation and text.
The remaining authors have made contributions to this paper that include, but
are not limited to, the construction of DECam and other aspects of collecting the data; data
processing and calibration; developing broadly used methods, codes, and simulations; running
the pipelines and validation tests; and promoting the science analysis.

\begin{thebibliography}{dummy}
%\bibliography
%\bibliographystyle{aasjournal}
% Y6 WZ paper
\bibitem[d'Assignies et al.(2025)]{y6wz} d'Assignies, W. et al.\ 2025
  (in preparation)

\bibitem[Bridle et al.(2002)]{bridle02}
Bridle, S. L., Crittenden, R., Melchiorri, A., Hobson, M. P., Kneissl, R., and Lasenby, A. N. 2002, \mnras, 335, 1193, 
doi:10.1046/j.1365-8711.2002.05709.x
 
\bibitem[Cordero et al.(2022)]{hyperrank} Cordero, J.~P., Harrison, I., Rollins, R.~P., et al.\ 2022, \mnras, 511, 2170. doi:10.1093/mnras/stac147

% lenses paper
\bibitem[Giannini et al.(2025)]{y6lenspz} Giannini, G. et al.\ 2025 (in preparation)

\bibitem[Hadzhiyska et al.(2020)]{hans} Hadzhiyska, B., Alonso, D., Nicola, A., et al.\ 2020, \jcap, 2020, 056. doi:10.1088/1475-7516/2020/10/056

\bibitem[Myles et al.(2021)]{y3pz}
  Myles, J. et al.\ 2021, \mnras, 505, 4249,
  doi:10.1093/mnras/stab1515
  
\bibitem[Giannini et al.(2025)]{y6maglim} Weaverdyck, N. et al.\ 2025 (in preparation)

\bibitem[Yin et al.(2025)]{y6pz} Yin, B. et al.\ 2025 (in preparation)
  
\bibitem[Zuntz et al.(2015)]{cosmosis}
  Zuntz, J. et al. 2015, Astronomy and Computing, 12, 45,
  doi:10.1016/j.ascom.2015.05.005

% Y6 modelling paper
\bibitem[Sanchez-Cid et al.(2025)]{y6model} Sanchez-Cid et al.\ 2025
  (in preparation)
  
\end{thebibliography}

\allauthors
\end{document}